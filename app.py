from dotenv import load_dotenv
import streamlit as st
from PyPDF2 import PdfReader
from langchain.text_splitter import CharacterTextSplitter
from langchain.vectorstores import FAISS
from langchain.chains.question_answering import load_qa_chain
from langchain.callbacks import get_openai_callback
from setup_api import setup_llm, setup_embeddings


def main():
    load_dotenv()
    st.set_page_config(page_title="Ask your PDF")
    st.header("Ask your PDF ğŸ’¬")
    
    # upload file
    pdf = st.file_uploader("Upload your PDF", type="pdf")
    
    # extract the text
    if pdf is not None:
      pdf_reader = PdfReader(pdf)
      text = ""
      for page in pdf_reader.pages:
        text += page.extract_text()
        
      # split into chunks
      text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
      )
      chunks = text_splitter.split_text(text)
      
      # create embeddings
      embeddings =  setup_embeddings() # ç»Ÿä¸€æ¥å£
      knowledge_base = FAISS.from_texts(chunks, embeddings)

      
      # æ£€æŸ¥ Session State æ˜¯å¦æœ‰å¯¹è¯å†å²è®°å½•
      if 'conversation_history' not in st.session_state:
          st.session_state.conversation_history = []

      # åˆå§‹åŒ–è¿­ä»£è®¡æ•°å™¨
      if 'iteration_count' not in st.session_state:
          st.session_state.iteration_count = 0

      # æ˜¾ç¤ºç”¨æˆ·è¾“å…¥å’Œå›ç­”
      while True:
          # ä¸ºæ¯æ¬¡å¾ªç¯è¿­ä»£ç”Ÿæˆä¸€ä¸ªå”¯ä¸€çš„ key
          unique_key = f'user_question_{st.session_state.iteration_count}'
          
          user_question = st.text_input("Ask a question about your PDF (type 'exit' to stop):", key=unique_key)
          
          # å¦‚æœç”¨æˆ·è¾“å…¥äº† 'exit' æˆ–è€…è¾“å…¥æ¡†ä¸ºç©ºï¼Œåˆ™é€€å‡ºå¾ªç¯
          if user_question.strip().lower() == 'exit' or not user_question.strip():
              break
          
          if user_question:  # ç¡®ä¿ç”¨æˆ·è¾“å…¥äº†é—®é¢˜
              docs = knowledge_base.similarity_search(user_question)

              llm = setup_llm() # ç»Ÿä¸€æ¥å£
              chain = load_qa_chain(llm, chain_type="stuff")
              with get_openai_callback() as cb:
                  response = chain.run(input_documents=docs, question=user_question)
                  # å­˜å‚¨å¯¹è¯å†å²
                  st.session_state.conversation_history.append((user_question, response))

              st.write(f"Q: {user_question}")
              st.write(f"A: {response}")
          
          # æ›´æ–°è¿­ä»£è®¡æ•°å™¨ï¼Œä¸ºä¸‹ä¸€æ¬¡å¾ªç¯å‡†å¤‡
          st.session_state.iteration_count += 1

      # æ˜¾ç¤ºå¯¹è¯å†å²
      st.header("Conversation History")
      for question, answer in st.session_state.conversation_history:
          st.write(f"Q: {question}")
          st.write(f"A: {answer}")
    

if __name__ == '__main__':
    main()
